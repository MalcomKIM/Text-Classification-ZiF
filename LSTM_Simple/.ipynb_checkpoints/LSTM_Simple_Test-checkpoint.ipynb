{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt='''\n",
    "Coronavirus: More pupils return to Scotland's schools\n",
    "7 minutes ago\n",
    "Share this with Facebook Share this with Messenger Share this with Twitter Share this with Email Share\n",
    "Related TopicsCoronavirus pandemic\n",
    "Image copyrightPA MEDIA\n",
    "Thousands more pupils returned to Scotland's schools on Wednesday morning - after almost five months away from the classroom.\n",
    "\n",
    "Schools have only been open to children of key workers since restrictions were put in place on 20 March.\n",
    "\n",
    "Many schools will have pupils return on a phased basis.\n",
    "\n",
    "Councils have been given some flexibility but the Scottish government wants all schools fully open by 18 August.\n",
    "On Tuesday the first pupils returned to school in the Borders and Shetland.\n",
    "\n",
    "Except for children of key workers, most of the country's 700,000 pupils have not been in class since schools closed in March.\n",
    "\n",
    "Pupils return to classrooms after lockdown\n",
    "Is it safe for Scotland's schools to reopen?\n",
    "What's your council's plan for the return to school?\n",
    "Has your child returned to school? Email haveyoursay@bbc.co.uk\n",
    "As students returned, additional hygiene and safety measures such as one-way systems were put in place.\n",
    "\n",
    "While there is no requirement for physical distancing between pupils, teachers should remain 2m apart from students or other adults.\n",
    "\n",
    "Older secondary pupils are also being encouraged to maintain distancing where possible if this does not hinder the return to full-time learning.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "my_sw = ['make', 'amp',  'news','new' ,'time', 'u','s', 'photos',  'get', 'say']\n",
    "def black_txt(token):\n",
    "    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2 and token not in my_sw\n",
    "  \n",
    "  \n",
    "def clean_txt(text):\n",
    "  clean_text = []\n",
    "  clean_text2 = []\n",
    "  text = re.sub(\"'\", \"\",text)\n",
    "  text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "  clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "  clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "  return \" \".join(clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=clean_txt(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = load_obj('word2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "words=tokenizer.tokenize(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = [word2idx[word] for word in words if word in word2idx]\n",
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23531,  3048, 24101,  9022,  4573, 14581, 29553,  5946,  4845,\n",
       "        24363, 23531,  4956, 16302, 22544,  4856, 15678, 23181, 23599,\n",
       "        23531, 22300, 29209,  5920, 20336, 22544, 23531,  4563, 22544,\n",
       "        23531,  8411,  2241, 25694, 22544,   276, 12790, 23187, 16709,\n",
       "        18905, 29056, 26224, 21420, 20321, 22387, 20162,  7535, 26450,\n",
       "        22224,  1146, 25694,   370, 18846, 23708,   769,  8543, 16104,\n",
       "         7535, 20682, 12283, 22544, 10451, 15199]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_data = pad_sequences([indexes], maxlen=60, value=29942)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('LSTM_Simple.h5')\n",
    "\n",
    "res=model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['ARTS', 'ARTS & CULTURE', 'BLACK VOICES', 'BUSINESS', 'COLLEGE',\n",
    "       'COMEDY', 'CRIME', 'CULTURE & ARTS', 'DIVORCE', 'EDUCATION',\n",
    "       'ENTERTAINMENT', 'ENVIRONMENT', 'FIFTY', 'FOOD & DRINK',\n",
    "       'GOOD NEWS', 'GREEN', 'HEALTHY LIVING', 'HOME & LIVING', 'IMPACT',\n",
    "       'LATINO VOICES', 'MEDIA', 'MONEY', 'PARENTING', 'PARENTS',\n",
    "       'POLITICS', 'QUEER VOICES', 'RELIGION', 'SCIENCE', 'SPORTS',\n",
    "       'STYLE', 'STYLE & BEAUTY', 'TASTE', 'TECH', 'TRAVEL', 'WEDDINGS',\n",
    "       'WEIRD NEWS', 'WELLNESS', 'WOMEN', 'WORLD NEWS', 'WORLDPOST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EDUCATION'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(res)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
