{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt='''\n",
    "Pupils get record GCSE grades as BTecs are pulled\n",
    "GCSE passes for England's pupils, in the most disrupted academic year in UK history, have risen dramatically.\n",
    "\n",
    "Grades have been awarded by schools, after exams were cancelled, and data shows 78.8% of papers were rated grade 4 or above. It was 69.9% in 2019.\n",
    "\n",
    "There was a rise of a quarter in the top grades - a 7 or above, which is equivalent to an A in the old system.\n",
    "\n",
    "The exams season has been dogged by chaos, with policy changes leading to grades being altered at the 11th hour.\n",
    "\n",
    "In the latest debacle BTec grades were pulled hours before pupils were to receive them although some schools are giving out grades, which were assessed by schools, anyway.\n",
    "\n",
    "And universities are still waiting for pupils' adjusted A-level results, while they attempt to squeeze as many as possible into the courses they have qualified for.\n",
    "\n",
    "Universities and the government have now agreed to honour all degree places - this year or next - to students who have obtained the right grades, but there are concerns about the funding of these.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "my_sw = ['make', 'amp',  'news','new' ,'time', 'u','s', 'photos',  'get', 'say']\n",
    "def black_txt(token):\n",
    "    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2 and token not in my_sw\n",
    "  \n",
    "  \n",
    "def clean_txt(text):\n",
    "  clean_text = []\n",
    "  clean_text2 = []\n",
    "  text = re.sub(\"'\", \"\",text)\n",
    "  text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "  clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "  clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "  return \" \".join(clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=clean_txt(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = load_obj('word2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "words=tokenizer.tokenize(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = [word2idx[word] for word in words if word in word2idx]\n",
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24205, 19526, 21745, 11229, 22747, 21474, 27096, 11229,  8769,\n",
       "        18844, 26219,  9013, 23689,  7658,  4374, 20514,  4362, 15176,\n",
       "        11229,   773, 12596, 15078,  6636, 11229, 21335, 12599, 21878,\n",
       "          782, 23531, 10946, 11229,  1527, 23531,  1136, 28044, 25507,\n",
       "        28888,   306, 15369, 22493,  1633, 25232, 16282, 20682,  5961,\n",
       "        21459, 28044, 11206,   518, 12483,  6804, 20321, 29722, 18293,\n",
       "        25694, 18733, 22702, 11229,  5402, 10465]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_data = pad_sequences([indexes], maxlen=60, value=29942)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_txt(text):\n",
    "  return TextBlob(text).sentiment[0]\n",
    "\n",
    "def subj_txt(text):\n",
    "  return  TextBlob(text).sentiment[1]\n",
    "\n",
    "def len_text(text):\n",
    "  if len(text.split())>0:\n",
    "         return len(set(clean_txt(text).split()))/ len(text.split())\n",
    "  else:\n",
    "         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=polarity_txt(test_txt)\n",
    "subj=subj_txt(test_txt)\n",
    "length=len_text(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subj</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.216883</td>\n",
       "      <td>0.394156</td>\n",
       "      <td>0.356383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity      subj    length\n",
       "0  0.216883  0.394156  0.356383"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data ={'polarity':[polarity],\n",
    "      'subj':[subj],\n",
    "      'length':[length]}\n",
    "\n",
    "df = pd.DataFrame (data, columns=['polarity','subj','length'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('Bi_GRU.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict([input_data[0].reshape(1,60),df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['ARTS', 'ARTS & CULTURE', 'BLACK VOICES', 'BUSINESS', 'COLLEGE',\n",
    "       'COMEDY', 'CRIME', 'CULTURE & ARTS', 'DIVORCE', 'EDUCATION',\n",
    "       'ENTERTAINMENT', 'ENVIRONMENT', 'FIFTY', 'FOOD & DRINK',\n",
    "       'GOOD NEWS', 'GREEN', 'HEALTHY LIVING', 'HOME & LIVING', 'IMPACT',\n",
    "       'LATINO VOICES', 'MEDIA', 'MONEY', 'PARENTING', 'PARENTS',\n",
    "       'POLITICS', 'QUEER VOICES', 'RELIGION', 'SCIENCE', 'SPORTS',\n",
    "       'STYLE', 'STYLE & BEAUTY', 'TASTE', 'TECH', 'TRAVEL', 'WEDDINGS',\n",
    "       'WEIRD NEWS', 'WELLNESS', 'WOMEN', 'WORLD NEWS', 'WORLDPOST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EDUCATION'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(res)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
